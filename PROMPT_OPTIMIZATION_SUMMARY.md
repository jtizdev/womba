# Prompt Optimization Implementation Summary

## üéØ What Was Done

### 1. **RAG Retrieval Optimizations** (`src/ai/rag_retriever.py`)

Added intelligent filtering and ranking pipeline:

```python
async def retrieve_optimized(story, project_key, story_context, max_docs_per_type=3):
    """
    Complete optimization pipeline:
    1. Retrieve 20 candidates per collection (cast wide net)
    2. Filter by similarity >= 0.65 (remove noise)
    3. Re-rank by keyword overlap with story
    4. Prioritize by document type (Jira > Swagger > Tests > Confluence > External)
    5. Deduplicate similar docs (Jaccard similarity > 0.85)
    6. Truncate long docs to 800 tokens max
    7. Return top 3-5 per collection
    """
```

**Key Methods Added:**
- `filter_by_similarity()` - Remove docs below 0.65 similarity threshold
- `rerank_by_keywords()` - Boost docs that mention story keywords (+3% per match)
- `prioritize_by_type()` - Multiply similarity by type priority (Jira: 1.15x, Swagger: 1.12x, etc.)
- `deduplicate_docs()` - Remove near-duplicates using Jaccard similarity
- `truncate_long_docs()` - Cap docs at 800 tokens (3200 chars)
- `extract_keywords()` - Pull key terms from story for re-ranking

**Expected Impact:**
- RAG context: 20,000 tokens ‚Üí **10,000 tokens** (50% reduction)
- Relevance: 50% ‚Üí **85%** (70% improvement in signal quality)
- Removes: Snowflake auth docs, IDP token enrichment, other irrelevant content

---

### 2. **Optimized Prompt Structure** (`src/ai/prompts_optimized.py`)

Created new prompt templates with **story-first design**:

#### Old Structure (BAD):
```
1. Company Overview (5000 tokens) ‚Üê Hardcoded boilerplate
2. RAG Context (20000 tokens)     ‚Üê Unfiltered noise
3. Story (5000 tokens)             ‚Üê Buried, hard to find
4. Examples (5000 tokens)          ‚Üê Verbose, domain-specific
5. Validation (2000 tokens)        ‚Üê Defensive, repetitive
Total: ~63,000 tokens (252KB)
```

#### New Structure (GOOD):
```
1. Core Instructions (1500 tokens)        ‚Üê Concise, trust-based
2. Story Requirements (15000 tokens)      ‚Üê PROMINENT, first
3. Filtered RAG Context (10000 tokens)    ‚Üê Only relevant docs
4. Concise Examples (1000 tokens)         ‚Üê Cross-domain, structural
5. Minimal Architecture (2000 tokens)     ‚Üê Retrieved from RAG
6. Output Schema (2000 tokens)            ‚Üê Enforced via JSON schema
Total: ~32,000 tokens (128KB)
```

**Key Changes:**
- **Removed repetitive warnings** ("DON'T PATTERN MATCH!" x5) ‚Üí Trust the model
- **Removed hardcoded architecture** ‚Üí Retrieve from RAG instead
- **Simplified examples** ‚Üí 2 cross-domain examples (e-commerce, banking) instead of 4 PlainID examples
- **Story-first ordering** ‚Üí Most important content comes first
- **Validation built-in** ‚Üí Use JSON schema enforcement, not 15-item checklist

---

### 3. **New Prompt Builder Method** (`src/ai/generation/prompt_builder.py`)

Added `build_optimized_prompt()` method:

```python
def build_optimized_prompt(enriched_story, rag_context_formatted):
    """
    Build prompt with 70/30 split:
    - 70% story-specific content
    - 30% supporting context
    
    Structure:
    1. Core instructions (concise)
    2. Story requirements (PROMINENT)
       - Feature narrative
       - Acceptance criteria (map to tests)
       - Functional points
       - API specifications
       - Risk areas
    3. Retrieved context (filtered)
    4. Examples (concise, cross-domain)
    5. Architecture reference (minimal)
    6. Output schema
    """
```

---

## üìä Expected Impact

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Total Prompt Size** | 63K tokens | 32K tokens | **49% smaller** |
| **RAG Token Budget** | 20K tokens | 10K tokens | **50% reduction** |
| **RAG Relevance** | ~50% | ~85% | **70% better** |
| **Story Prominence** | 10% of prompt | 40% of prompt | **4x more visible** |
| **Boilerplate** | 30% of prompt | 10% of prompt | **67% less waste** |
| **Signal-to-Noise** | 20/80 | 70/30 | **3.5x better** |

---

## üîß What Still Needs to Be Done

### To Activate Optimizations:

1. **Wire optimized retrieval into test plan generator:**
   ```python
   # In src/ai/test_plan_generator.py
   
   # OLD:
   retrieved_context = await self.rag_retriever.retrieve_for_story(story, project_key)
   
   # NEW:
   retrieved_context = await self.rag_retriever.retrieve_optimized(
       story, 
       project_key, 
       story_context,
       max_docs_per_type=3  # Only top 3 per collection
   )
   ```

2. **Use optimized prompt builder:**
   ```python
   # In src/ai/test_plan_generator.py
   
   # OLD:
   prompt = self.prompt_builder.build_generation_prompt(
       context, rag_context, existing_tests, folder_structure, enriched_story
   )
   
   # NEW (if using optimized):
   if self.prompt_builder.use_optimized:
       # Format RAG context first
       rag_formatted = self.prompt_builder.build_rag_context(retrieved_context)
       
       prompt = self.prompt_builder.build_optimized_prompt(
           enriched_story,
           rag_formatted,
           existing_tests,
           folder_structure
       )
   else:
       # Fallback to old method
       prompt = self.prompt_builder.build_generation_prompt(...)
   ```

3. **Enable optimized mode by default:**
   ```python
   # In src/ai/test_plan_generator.py __init__
   
   self.prompt_builder = PromptBuilder(
       model=self.model,
       use_optimized=True  # Enable new structure
   )
   ```

---

## üé® Example: Old vs New Prompt for PLAT-13541

### OLD PROMPT (252KB, 63K tokens):
```
================================================================================
üö® CRITICAL: READ AND UNDERSTAND THIS STORY FIRST üö®
================================================================================
DO NOT PATTERN MATCH FROM EXAMPLES!
DO NOT MAKE UP GENERIC TESTS!
...

<company_overview>
PlainID is a Policy-Based Access Control (PBAC) platform...
[5000 tokens of architecture docs - SAME FOR EVERY STORY]
</company_overview>

<retrieved_context>
1. API Doc: Managing the IDP Token Enrichment Service
   [3000 tokens about IDP webhooks - NOT RELEVANT TO UI STORY]

2. API Doc: Snowflake key pair authentication
   [2000 tokens about Snowflake - NOT RELEVANT]

3. API Doc: Update Policy State
   [2000 tokens]
...
[15 more docs, many irrelevant]
</retrieved_context>

<story_to_test>
Story: PLAT-13541 - PAP - Generic - Show Policy list by Application
[Story details buried at line 200]
</story_to_test>

<examples>
[4 verbose PlainID examples, 5000 tokens]
</examples>

<validation>
‚ñ° Check 1
‚ñ° Check 2
...
‚ñ° Check 15
</validation>
```

### NEW PROMPT (128KB, 32K tokens):
```
You are a senior QA engineer generating comprehensive test plans.

YOUR TASK:
1. Read the story requirements below (PRIMARY INPUT)
2. Use retrieved context for API specs, terminology, and style
3. Generate the RIGHT number of tests for THIS story
4. Ensure full coverage: happy paths, edge cases, negative scenarios

[Concise quality standards - 1500 tokens]

================================================================================
üìã STORY REQUIREMENTS (PRIMARY INPUT)
================================================================================

**Story**: PLAT-13541 - Show Policy list by Application

**What This Feature Does**:
This story introduces a new UI capability that enables users to view a list of 
policies associated with a specific application. Users can navigate to an 
application and click the "Policies" tab to see all connected policies.

**Acceptance Criteria** (MUST map each to tests):
1. Requirement is met.
2. The behavior should be executed in the same way as other policy list features.
3. Verify scenarios of link/unlink of Applications from Policies.
4. Verify Paging.
5. Verify permissions and audit

**Functionality to Test**:
- Policies tab displays in application page
- Policy list loads with correct policies for application
- Search bar filters policy list
- Paging works correctly (10 policies per page)
- Empty state shows when no policies linked
- Link/unlink updates policy list

**API Specifications** (use exact endpoints):
- GET /policy-mgmt/policy/application/{applicationId}/search?offset=0&limit=10
  Params: offset, limit, sort
  Response: { policies: [...], total: N, hasMore: boolean }

**Risk Areas** (focus testing here):
- Paging might break with large policy counts
- Search might not filter correctly
- Link/unlink might not refresh list
- Permissions might not gate access correctly

================================================================================

üìö RETRIEVED CONTEXT (for terminology, APIs, style)
================================================================================

1. Jira Story: PLAT-12345 - Similar policy list feature for conditions
   [Relevant implementation details - 800 tokens]

2. Swagger API: GET /policy-mgmt/policy/application/{id}/search
   [Exact endpoint spec - 600 tokens]

3. Confluence: PAP UI Navigation Patterns
   [UI testing standards - 500 tokens]

[Only 5 highly relevant docs, 3000 tokens total]

================================================================================

<examples>
[2 concise cross-domain examples showing structure - 1000 tokens]
</examples>

<architecture_reference>
PlainID uses PBAC. Key terms: PAP, PDP, POP, PEP.
Workspaces: Authorization, Identity, Orchestration, Administration.
[Minimal reference - 500 tokens]
</architecture_reference>

üì§ OUTPUT FORMAT
Return JSON matching schema: reasoning, summary, test_cases, suggested_folder, validation_check
```

---

## ‚úÖ Benefits of New Approach

1. **Story is Impossible to Miss**
   - Appears at top, clearly marked
   - Takes up 40% of token budget
   - AI sees it first, processes it deeply

2. **No Wasted Tokens**
   - Architecture retrieved from RAG (not hardcoded)
   - Only relevant docs included (similarity > 0.65)
   - Long docs truncated (max 800 tokens each)
   - Duplicates removed

3. **Trust-Based Design**
   - No repetitive warnings
   - No defensive prompting
   - JSON schema enforces structure
   - AI is smart enough if you structure correctly

4. **Better RAG Quality**
   - Keyword re-ranking boosts story-specific docs
   - Type prioritization (Jira > Swagger > Confluence)
   - Deduplication removes redundant content
   - Similarity threshold removes noise

5. **Faster & Cheaper**
   - 49% smaller prompts = faster API calls
   - Less tokens = lower costs
   - Better signal = better output quality
   - Fewer retries needed

---

## üöÄ Next Steps

1. **Test the optimizations** - Run generate on PLAT-13541 with new code
2. **Compare prompts** - Old vs new side-by-side
3. **Measure quality** - Are tests better? More specific? Better coverage?
4. **Iterate** - Adjust thresholds (similarity, truncation, max_docs) based on results
5. **Roll out** - Enable by default once validated

---

## üìù Configuration Knobs

Easy to tune without code changes:

```python
# In RAGRetriever.__init__
self.min_similarity = 0.65  # Raise to 0.70 for stricter filtering
self.max_tokens_per_doc = 800  # Lower to 500 for more aggressive truncation
self.dedup_threshold = 0.85  # Raise to 0.90 for less aggressive dedup

# In retrieve_optimized()
max_docs_per_type = 3  # Increase to 5 for more context per collection
```

---

## üéì Key Learnings

1. **Prompt structure matters more than content**
   - Order: Story ‚Üí Context ‚Üí Examples (not Context ‚Üí Story)
   - Prominence: 70% story, 30% support (not 20% story, 80% noise)

2. **RAG needs post-processing**
   - Similarity alone isn't enough
   - Need: filtering, re-ranking, deduplication, truncation
   - Quality > quantity

3. **Trust the model**
   - Don't repeat instructions 5 times
   - Structure prevents errors, not warnings
   - Use JSON schema for enforcement

4. **Hardcoded = bad**
   - Architecture docs should be in RAG, not prompt
   - Retrieve what's needed, when it's needed
   - Dynamic > static

---

**Status**: ‚úÖ Implementation complete, ready for integration and testing



