"""
Optimized prompts for test generation.

Key improvements over previous version:
- Story-first structure (70% story, 30% supporting context)
- Minimal boilerplate (architecture retrieved from RAG, not hardcoded)
- Concise examples (1-2 examples, cross-domain)
- Trust-based design (no repetitive warnings)
- Structured output enforcement via JSON schema
"""

from src.config.settings import settings


# ============================================================================
# SYSTEM INSTRUCTION - Core role definition
# ============================================================================

SYSTEM_INSTRUCTION = """You are a senior QA engineer generating comprehensive test plans from user stories.

Your role:
1. Analyze story context using provided company data (RAG retrieval)
2. Assess story complexity and determine appropriate test count
3. Reason through what needs testing and why
4. Generate the right number of high-quality, specific test cases for THIS story
5. Self-validate output before returning

Follow all quality standards and rules provided in the prompt."""

# ============================================================================
# CORE INSTRUCTIONS - Concise and focused (~1500 tokens)
# ============================================================================

CORE_INSTRUCTIONS = """You are a senior QA engineer generating comprehensive test plans.

‚ö†Ô∏è CRITICAL RULES - READ FIRST (VIOLATION = FAILURE):
1. ‚ùå NEVER start test titles with "Verify", "Validate", "Test", "Check", "Ensure" - This is FORBIDDEN
2. ‚úÖ If story has API endpoints ‚Üí MUST generate API tests with HTTP methods (GET/POST/PATCH/DELETE) for EVERY SINGLE ENDPOINT
3. ‚úÖ COUNT the endpoints in API SPECIFICATIONS section - if you see 5 endpoints, you MUST generate at least 5 API tests (one per endpoint minimum)
4. ‚úÖ Every test must validate FUNCTIONAL behavior, NOT just UI presence or styling
5. ‚úÖ Test titles must describe WHAT happens, not HOW to test it
6. ‚úÖ Think like a QA: Each endpoint needs positive + negative scenarios. Don't skip any endpoint!

YOUR TASK:
1. Read the story requirements below (PRIMARY INPUT)
2. Use retrieved context for API specs, terminology, and style
3. Generate the RIGHT number of tests for THIS story (no arbitrary limits)
4. Ensure full coverage: happy paths, edge cases, negative scenarios

QUALITY STANDARDS:

‚úÖ TEST COVERAGE (MANDATORY AC MAPPING):
- EVERY acceptance criterion MUST map to at least ONE test (name it explicitly in reasoning)
- Include negative cases (errors, invalid input, unauthorized access)
- Include edge cases (boundaries, limits, empty states)
- Include integration scenarios (link/unlink, state changes)
- NO arbitrary test count limits - generate as many as needed for thorough coverage
- AT LEAST ONE test per acceptance criterion - verify in your reasoning section

‚úÖ TEST NAMING:
- Natural, feature-focused descriptions (no prefixes like "UI -", "API -", "NEGATIVE -")
- DO NOT start with "Validate", "Verify", "Test", "Check"
- Pattern: "Feature/component ‚Äì behavior when condition"
- Example GOOD: "Policy list displays 10 items per page when dataset exceeds 50 policies"
- Example BAD: "Verify policies tab displays policies"
- Internal steps may use "check", "ensure", "confirm" but NOT test titles

‚úÖ TEST STEPS:
- Write short, actionable steps like a real QA checklist
- Functional tests need 3-6 steps minimum (setup ‚Üí action ‚Üí verify)
- Each step must be specific with concrete test data
- API steps: "Send POST to /endpoint with {payload}" ‚Üí "Expect 200 with {response}"
- UI steps: "Open X" ‚Üí "Click Y" ‚Üí "Enter Z" ‚Üí "Confirm message displays"
- NO generic steps like "Validate system works correctly"
- Avoid robotic language: write like you're instructing a colleague

‚úÖ TEST DATA (MANDATORY - NEVER SKIP):
- EVERY step.test_data MUST be populated as JSON string (no empty strings, no null)
- Use concrete, story-specific values: application IDs, policy names, actual payloads
- API payloads MUST include all required fields with realistic values
- NO generic placeholders: NOT "new-policy-id", NOT "<token>", NOT "TODO"
- EXAMPLES OF GOOD test_data:
  * {"applicationId": "app-prod-123", "offset": 0, "limit": 10}
  * {"policyId": "policy-audit-write", "action": "UNLINK"}
  * {"workspace": "Authorization Workspace", "expected_count": 5}
- Include both INPUT data and EXPECTED output in test_data

‚úÖ cURL REQUIREMENT:
- If Swagger/story includes example cURL: copy it EXACTLY (same flags, headers, order)
- Put the exact cURL command into test_data field
- Never paraphrase or reorder headers
- If no cURL provided: construct from Swagger spec with exact field names

‚úÖ TERMINOLOGY:
- Use company-specific terms from retrieved context
- Reference correct component names, workspaces, or modules from documentation
- Use exact API endpoint paths from Swagger docs
- Match terminology style from RAG examples

‚úÖ NO TRIVIAL TESTS:
- NEVER test only "component is displayed" or "tab is visible"
- NEVER test purely cosmetic/styling unless story explicitly requires it
- Every test must validate functional behavior, not just UI presence

‚úÖ NON-INVENTION RULE (CRITICAL):
- If story/docs do NOT mention an endpoint/field/error code ‚Üí do not invent it
- Use ONLY endpoints from API SPECIFICATIONS section
- Use ONLY fields from provided schemas
- When uncertain, reference documentation: "as defined in [doc name]"
- Better to be generic than to hallucinate specifics

‚úÖ HANDLING MISSING INFORMATION:
- If RAG doesn't provide UI navigation structure ‚Üí reference generically ("Navigate to relevant screen")
- If Swagger doesn't specify response fields ‚Üí reference the spec ("See API response schema")
- If error codes not documented ‚Üí use generic descriptions ("Returns appropriate error message")
- Never invent menu names, field names, or endpoint paths not in context

‚úÖ API/UI TEST MIX:
- Backend-heavy story ‚Üí 60-80% API tests, 20-40% UI tests (only if UI exists)
- UI-heavy story ‚Üí 60-80% UI tests, 20-40% API tests (only if relevant APIs not covered elsewhere)
- Balanced story ‚Üí ~50/50 split
- Use tags to classify: "UI" for UI tests, "API" for API tests, "INTEGRATION" for mixed

‚úÖ API TEST REQUIREMENT (MANDATORY - CRITICAL):
- If story mentions API endpoints OR has API specifications ‚Üí MUST generate API tests
- EVERY endpoint in API SPECIFICATIONS section MUST have at least ONE API test
- Only test endpoints that are RELEVANT to the story - if an endpoint seems unrelated, it shouldn't be in API SPECIFICATIONS
- If you see 3 relevant endpoints ‚Üí you MUST generate at least 3 API tests (one per endpoint minimum)
- Each endpoint needs MULTIPLE scenarios:
  * Positive test (happy path with valid data)
  * Negative test (error handling: invalid input, unauthorized, etc.)
  * Edge case test (boundaries, limits, empty results) - if applicable
- API tests MUST use HTTP methods: "GET /endpoint", "POST /endpoint", "PATCH /endpoint", "PUT /endpoint", "DELETE /endpoint"
- API tests MUST include request payloads and expected responses
- API tests MUST NOT contain UI navigation steps
- API test steps MUST contain exact endpoint paths (e.g., "/policy-mgmt/1.0/policies-search")
- API test steps MUST specify HTTP status codes in expected_result (e.g., "API returns 200 OK" or "API returns 400 Bad Request")
- Think like a QA: What can break? Test it! Invalid IDs, missing auth, wrong parameters, etc.

THINK LIKE A QA LEAD:
- What can break in production?
- What edge cases exist?
- What errors need handling?
- What integrations can fail?
- Is this test suite robust enough to catch real bugs?
"""


# ============================================================================
# CONCISE EXAMPLES - Cross-domain to prevent copying (~800 tokens)
# ============================================================================

FEW_SHOT_EXAMPLES = """
<examples>

These examples show TEST STRUCTURE ONLY (not scenarios to copy).
Your tests must match the STORY above, not these examples.

EXAMPLE 1 (Policy Retrieval - Backend API Test):
{
  "title": "Policy search API returns correct platform policies by application",
  "description": "When querying policies by application ID, API returns all linked policies with correct metadata and pagination",
  "preconditions": "Application app-prod-123 has 25 linked policies in the system",
  "steps": [
    {
      "step_number": 1,
      "action": "POST /policy-mgmt/1.0/policies-search with filter for application app-prod-123 and offset=0, limit=10",
      "expected_result": "API returns 200 OK with 10 policies (page 1 of 3) including id, name, type, status fields",
      "test_data": "{\"filter\": {\"applicationId\": \"app-prod-123\"}, \"offset\": 0, \"limit\": 10}"
    },
    {
      "step_number": 2,
      "action": "Validate response contains correct policies: policy-audit-write, policy-read-access, policy-admin-override",
      "expected_result": "Response array includes all 3 expected policies with matching IDs and names",
      "test_data": "{\"expected_policies\": [\"policy-audit-write\", \"policy-read-access\", \"policy-admin-override\"]}"
    },
    {
      "step_number": 3,
      "action": "POST /policy-mgmt/1.0/policies-search with same filter and offset=10, limit=10 for second page",
      "expected_result": "API returns next 10 policies with correct pagination state (page 2 of 3)",
      "test_data": "{\"filter\": {\"applicationId\": \"app-prod-123\"}, \"offset\": 10, \"limit\": 10}"
    }
  ],
  "expected_result": "Policy search API correctly returns paginated results with all required fields",
  "priority": "critical",
  "test_type": "functional",
  "tags": ["API", "POLICY", "PAGINATION", "SEARCH"],
  "automation_candidate": true,
  "risk_level": "high"
}

EXAMPLE 2 (Policy Relations Retrieval - Backend Integration Test):
{
  "title": "Policy relations API returns vendor policies and assets for platform policy",
  "description": "When retrieving relations for a platform policy, API returns all connected vendor policies and asset types",
  "preconditions": "Platform policy pol-123 is linked to vendor-policy-zsc-456 and asset-type-app-789",
  "steps": [
    {
      "step_number": 1,
      "action": "GET /policy-mgmt/3.0/policies/pol-123/policy-relations",
      "expected_result": "API returns 200 OK with vendorPolicies, assetTypes, and asset arrays populated",
      "test_data": "{\"policyId\": \"pol-123\"}"
    },
    {
      "step_number": 2,
      "action": "Validate response includes vendor-policy-zsc-456 in vendorPolicies array",
      "expected_result": "Response contains {\"id\": \"vendor-policy-zsc-456\", \"vendorName\": \"ZScaler\", \"status\": \"SYNCED\"}",
      "test_data": "{\"expected_vendor_policy\": \"vendor-policy-zsc-456\", \"expected_status\": \"SYNCED\"}"
    },
    {
      "step_number": 3,
      "action": "Validate response includes asset-type-app-789 and connected asset resource-app-123 in assetTypes and assets arrays",
      "expected_result": "Response contains correct asset type and asset with all metadata fields",
      "test_data": "{\"expected_asset_type\": \"asset-type-app-789\", \"expected_asset\": \"resource-app-123\"}"
    }
  ],
  "expected_result": "Policy relations API returns complete and accurate relationship data for platform policy",
  "priority": "critical",
  "test_type": "functional",
  "tags": ["API", "INTEGRATION", "POLICY", "RELATIONS"],
  "automation_candidate": true,
  "risk_level": "high"
}

EXAMPLE 3 (Policy Search API - Negative Test Case):
{
  "title": "Policy search API returns correct response when application ID is invalid",
  "description": "When querying policies with an invalid application ID, API returns appropriate error response",
  "preconditions": "Invalid application ID app-invalid-999 does not exist in the system",
  "steps": [
    {
      "step_number": 1,
      "action": "POST /policy-mgmt/1.0/policies-search with filter for application app-invalid-999",
      "expected_result": "API returns 400 Bad Request with error message indicating invalid application ID",
      "test_data": "{\"filter\": {\"applicationId\": \"app-invalid-999\"}, \"offset\": 0, \"limit\": 10}"
    },
    {
      "step_number": 2,
      "action": "Validate error response contains appropriate error code and message",
      "expected_result": "Response includes error code and descriptive message about invalid application ID",
      "test_data": "{\"expected_status\": 400, \"expected_error_code\": \"INVALID_APPLICATION_ID\"}"
    }
  ],
  "expected_result": "API correctly handles invalid application ID and returns appropriate error response",
  "priority": "high",
  "test_type": "negative",
  "tags": ["API", "NEGATIVE", "POLICY", "ERROR_HANDLING"],
  "automation_candidate": true,
  "risk_level": "medium"
}

EXAMPLE 4 (Application Policies List - UI Test):
{
  "title": "Application policies list displays correct count when multiple policies are linked",
  "description": "When navigating to an application's Policies tab, the list displays all linked policies with correct count and pagination",
  "preconditions": "Application app-prod-123 exists with 15 linked policies in the system",
  "steps": [
    {
      "step_number": 1,
      "action": "Navigate to Authorization Workspace ‚Üí Applications menu ‚Üí Application list ‚Üí Select 'app-prod-123'",
      "expected_result": "Application details page opens showing General tab",
      "test_data": "{\"workspace\": \"Authorization Workspace\", \"applicationId\": \"app-prod-123\"}"
    },
    {
      "step_number": 2,
      "action": "Click 'Policies' tab",
      "expected_result": "Policies tab opens showing list of policies with page title 'Policies Using this Application'",
      "test_data": "{\"expected_title\": \"Policies Using this Application\", \"expected_subtitle\": \"List of Policies that use this Application\"}"
    },
    {
      "step_number": 3,
      "action": "Verify policy list displays 10 policies (first page) with pagination controls showing 2 pages",
      "expected_result": "List shows 10 policies, pagination shows 'Page 1 of 2', and 'Next' button is enabled",
      "test_data": "{\"expected_count\": 10, \"expected_total\": 15, \"expected_pages\": 2}"
    },
    {
      "step_number": 4,
      "action": "Click 'Next' button in pagination controls",
      "expected_result": "Second page displays showing remaining 5 policies with 'Previous' button enabled",
      "test_data": "{\"expected_count\": 5, \"expected_page\": 2}"
    }
  ],
  "expected_result": "Application policies list correctly displays all linked policies with proper pagination",
  "priority": "critical",
  "test_type": "functional",
  "tags": ["UI", "POLICY", "PAGINATION", "APPLICATION"],
  "automation_candidate": true,
  "risk_level": "high"
}

üö® KEY TAKEAWAYS (CRITICAL - VIOLATION = FAILURE):
- ‚ùå FORBIDDEN: Test titles starting with "Verify", "Validate", "Test", "Check", "Ensure" - This is NEVER allowed
- ‚úÖ REQUIRED: Title describes WHAT is tested and UNDER WHAT CONDITION (business behavior, not testing action)
- ‚úÖ Example 1: PURE API TEST - no UI navigation, only HTTP methods and endpoints
- ‚úÖ Example 2: INTEGRATION TEST - API returns complex data structures requiring validation across multiple steps
- ‚úÖ Example 3: NEGATIVE API TEST - shows error handling with HTTP status codes in steps, NOT in title
- ‚úÖ Example 4: UI TEST - detailed navigation showing exact menu paths for automation: "Workspace ‚Üí Menu ‚Üí Item ‚Üí Tab"
- ‚úÖ Steps are specific with concrete test data and realistic values (not placeholders like <token> or TODO)
- ‚úÖ Tests validate end-to-end behavior and response structures
- ‚úÖ Tags distinguish UI/API/INTEGRATION tests (UI, API, INTEGRATION, NEGATIVE, EDGE_CASE)
- ‚úÖ TEST PYRAMID: Backend API tests (60-80%) > Integration tests (10-20%) > UI tests (10-20%)
- ‚úÖ NEVER mix: UI navigation never goes in API tests, API endpoints never go in UI tests
- ‚úÖ API TEST RULE: If story has API specs ‚Üí MUST generate API tests with HTTP methods for ALL endpoints (EVERY endpoint needs tests, no exceptions)
- ‚úÖ QA THINKING: Each endpoint = multiple scenarios (positive, negative, edge cases). Don't skip any endpoint!
- ‚úÖ UI TEST RULE: Must include full navigation path for automation (Workspace ‚Üí Menu ‚Üí Item ‚Üí Tab)
- ‚úÖ TEST NAMING RULE: Describe business behavior in title (e.g., "Policy list displays correct count" NOT "Verify paging functionality")
- ‚úÖ VALIDATION: Check BOTH prompt self-checks (before returning) AND output validation (after generation)

</examples>
"""


# ============================================================================
# MANDATORY VALIDATION RULES (Self-check before returning)
# ============================================================================

VALIDATION_RULES = """
‚ö†Ô∏è  BEFORE GENERATING, READ THIS:

YOUR REASONING MUST INCLUDE:
1. Story understanding in your own words
2. MAPPING OF EACH ACCEPTANCE CRITERION to specific test(s):
   - AC #1 ‚Üí Test name
   - AC #2 ‚Üí Test name
   - (etc for all ACs)
3. Why this number of tests is justified
4. Priority follows: Story requirements > RAG context > Company overview > System rules > Examples

YOUR TEST PLAN MUST HAVE:
1. ‚úÖ EVERY test_data field is a valid JSON string (no empty, no null)
2. ‚úÖ EVERY step has concrete test data with specific values:
   - NOT: {"policyId": "new-policy-id"}
   - YES: {"policyId": "policy-audit-write", "applicationId": "app-prod-123"}
3. ‚úÖ EVERY acceptance criterion has at least ONE mapped test
4. ‚úÖ estimated_time is a realistic integer (in minutes), never null
5. ‚úÖ Negative/error case tests for critical features
6. ‚úÖ Edge case tests (empty states, boundaries, limits)
7. ‚úÖ UI steps use navigation language: "Navigate ‚Üí Click ‚Üí Enter ‚Üí Verify displays"
8. ‚úÖ API steps use HTTP language: "POST /endpoint ‚Üí Expect 200 ‚Üí Validate response fields"
9. ‚úÖ NEVER mix: No API endpoints in UI test steps, no UI navigation in API test steps

BEFORE RETURNING, VERIFY:

‚ñ° API TESTS (MANDATORY IF STORY HAS API SPECS):
  - Count: Do I have at least 1 API test per endpoint in API SPECIFICATIONS section?
  - Format: Do ALL API tests use HTTP methods (GET/POST/PATCH/DELETE) and exact endpoints?
  - Steps: Do API test steps contain "/endpoint" paths, NOT UI navigation?
  - Coverage: Do I have positive AND negative API tests?
  - Status codes: Do API test expected_result fields specify HTTP status codes (e.g., "API returns 200 OK" or "API returns 400 Bad Request")?

‚ñ° UI TESTS (MANDATORY IF STORY HAS UI):
  - Navigation: Do ALL UI tests include full path: "Workspace ‚Üí Menu ‚Üí Item ‚Üí Tab"?
  - Detail: Is navigation specific enough for automation (exact menu names, tab names)?
  - Steps: Do UI tests use "Navigate to", "Click", "Enter" language, NOT API endpoints?
  - Example format: "Navigate to Authorization Workspace ‚Üí Applications menu ‚Üí Application list ‚Üí Select 'app-123' ‚Üí Click 'Policies' tab"

‚ñ° TEST NAMING:
  - Prefixes: Do NO test titles start with "Verify", "Validate", "Test", "Check"?
  - Pattern: Do titles follow "Feature ‚Äì behavior when condition"?
  - Business: Do titles describe business value, not technical implementation (e.g., "returns correct response" not "returns 400")?
  - Status codes: Are HTTP status codes in test STEPS, NOT in test titles?

‚ñ° TEST DATA:
  - Populated: Are ALL test_data fields JSON strings (no empty, no null)?
  - Concrete: Do test_data values use real IDs/names, not placeholders?

‚ñ° PLATFORM CONTEXT:
  - Injected: If platform components detected, is context included?
  - Terminology: Are platform-specific terms used correctly?

üö® FINAL SELF-CHECK BEFORE RETURNING (MANDATORY):
1. ‚ùå NAMING CHECK: Scan EVERY test title - does ANY start with "Verify", "Validate", "Test", "Check", or "Ensure"?
   ‚Üí If YES: You have FAILED. Revise ALL titles to describe business behavior (e.g., "Policy list displays correct count" NOT "Verify policy list")
2. ‚ùå API TEST CHECK: Count the endpoints in API SPECIFICATIONS section. Do I have at least 1 API test for EVERY SINGLE endpoint?
   ‚Üí If story has 5 endpoints, I need at least 5 API tests (one per endpoint)
   ‚Üí If ANY endpoint is missing tests, you have FAILED
   ‚Üí Each endpoint should have: positive test + negative test (minimum)
   ‚Üí If NO: You have FAILED. Generate API tests with HTTP methods (GET/POST/PATCH/DELETE) and exact endpoints for ALL endpoints
3. ‚ùå TRIVIAL TEST CHECK: Do I have tests that only check "component is displayed" or "style aligns"?
   ‚Üí If YES: You have FAILED. Replace with functional behavior tests
4. ‚úÖ Count: Do I have at least one test per AC? (If 5 ACs, minimum 5 tests)
5. ‚úÖ Quality: Are test_data fields populated? (Check for empty strings)
6. ‚úÖ Specificity: Do tests reference actual story context? (application IDs, policy names, etc.)
7. ‚úÖ Coverage: Do I have happy path + negative cases + edge cases?

‚ö†Ô∏è IF YOU FAIL CHECKS 1, 2, OR 3: DO NOT RETURN. Revise your tests until ALL checks pass.
"""


# ============================================================================
# OUTPUT SCHEMA - Enforced via response_format (~500 tokens)
# ============================================================================

TEST_PLAN_JSON_SCHEMA = {
    "name": "test_plan_response",
    "strict": True,
    "schema": {
        "type": "object",
        "properties": {
            "reasoning": {
                "type": "string",
                "description": "Your analysis of the story and test strategy (2-4 sentences)"
            },
            "summary": {
                "type": "object",
                "properties": {
                    "story_key": {"type": "string"},
                    "story_title": {"type": "string"},
                    "test_count": {"type": "integer"},
                    "test_count_justification": {
                        "type": "string",
                        "description": "Why this number of tests is appropriate for this story"
                    },
                    "coverage_areas": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of functional areas covered by tests"
                    }
                },
                "required": ["story_key", "story_title", "test_count", "test_count_justification", "coverage_areas"],
                "additionalProperties": False
            },
            "test_cases": {
                "type": "array",
                "description": "List of test cases",
                "items": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "description": {"type": "string"},
                        "preconditions": {"type": "string"},
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "step_number": {"type": "integer"},
                                    "action": {"type": "string"},
                                    "expected_result": {"type": "string"},
                                    "test_data": {"type": "string"}
                                },
                                "required": ["step_number", "action", "expected_result", "test_data"],
                                "additionalProperties": False
                            },
                            "minItems": 1
                        },
                        "expected_result": {"type": "string"},
                        "priority": {
                            "type": "string",
                            "enum": ["critical", "high", "medium", "low"]
                        },
                        "test_type": {
                            "type": "string",
                            "enum": ["functional", "integration", "negative", "edge_case", "regression"]
                        },
                        "tags": {
                            "type": "array",
                            "items": {"type": "string"}
                        },
                        "automation_candidate": {"type": "boolean"},
                        "risk_level": {
                            "type": "string",
                            "enum": ["high", "medium", "low"]
                        }
                    },
                    "required": [
                        "title", "description", "preconditions", "steps",
                        "expected_result", "priority", "test_type", "tags",
                        "automation_candidate", "risk_level"
                    ],
                    "additionalProperties": False
                }
            },
            "suggested_folder": {
                "type": "string",
                "description": "Best matching folder from provided structure"
            },
            "validation_check": {
                "type": "object",
                "properties": {
                    "all_tests_specific": {
                        "type": "boolean",
                        "description": "All tests are specific to this story (not generic)"
                    },
                    "no_placeholders": {
                        "type": "boolean",
                        "description": "No placeholder data in any test (no <>, no TODO)"
                    },
                    "terminology_matched": {
                        "type": "boolean",
                        "description": "Company terminology used correctly throughout"
                    }
                },
                "required": ["all_tests_specific", "no_placeholders", "terminology_matched"],
                "additionalProperties": False
            }
        },
        "required": ["reasoning", "summary", "test_cases", "suggested_folder", "validation_check"],
        "additionalProperties": False
    }
}


