"""
Refactored QA-focused prompts using modern prompt engineering.

Key improvements:
- 50% token reduction through consolidation
- Chain-of-thought reasoning with visible output
- Structured JSON schema enforcement
- Company-agnostic (no hardcoded examples)
- Clear priority hierarchy
- XML tags for section clarity
"""

# ============================================================================
# COMPANY OVERVIEW - PlainID Platform Context (~400 tokens)
# ============================================================================

COMPANY_OVERVIEW = """
<plainid_overview>

PlainID is a Policy-Based Access Control (PBAC) platform. You MUST use PlainID-specific terminology in ALL tests.

CORE ARCHITECTURE (use these exact terms):
- PAP (Policy Administration Point): Web UI where policies are authored, validated, and managed
- PDP (Policy Decision Point): Runtime engine that evaluates authorization requests
- PEP (Policy Enforcement Point): Client-side component in applications that calls PDP
- POP (Policy Object Point): Stores deployed policies for PDP runtime
- PIPs (Policy Information Points): Data sources that enrich policy evaluation context
- Authorizers: Data connectors (IDP, Database, API, File-based) that feed PIPs

HIERARCHY (always reference correctly):
Tenant ‚Üí Environment (dev/test/prod) ‚Üí Workspace ‚Üí Policy/Asset/Authorizer
- Tenants contain multiple environments
- Each environment has its own POPs, authorizers, API clients
- Policies are promoted across environments (dev ‚Üí test ‚Üí prod)

WORKSPACES (use correct workspace names):
- Identity Workspace: Manage identity sources, templates, matchers, token enrichment, dynamic groups
- Policy Workspace: Author policies, define assets, scopes, conditions, approval workflows
- Authorization Workspace: Configure POPs, policy overrides, deny reasons, runtime enforcement
- Orchestration Workspace: Manage deployments, vendor synchronization, environment coordination

KEY ENTITIES (use exact terminology):
- Policy: Authorization rules with conditions, subjects, resources, actions
- Asset: Resource being protected (e.g., API endpoint, document, data record)
- Asset Type: Category of assets with common attributes
- Subject: User or entity requesting access
- Resource: What is being accessed
- Action: What operation is being performed (read, write, delete, etc.)
- Scope: Logical grouping of policies
- Authorizer: Data source connector (IDP Authorizer, SQL Authorizer, API Authorizer, etc.)
- Vendor ID: Unique identifier for policy objects across environments
- Policy Promotion: Moving policies between environments while preserving vendor IDs
- Deny Reason: Structured explanation when access is denied
- Entitlement: Subject's permitted actions on resources
- Token Enrichment: Adding policy data to identity tokens (JWT, SAML)
- Cache Invalidation: Clearing cached identity/policy data after changes

RUNTIME FLOW (reference in integration tests):
1. PEP sends authorization request to PDP: {subject, resource, action, context}
2. PDP retrieves policies from POP
3. PDP enriches request with data from PIPs/Authorizers
4. PDP evaluates policies and returns decision: Permit/Deny with deny reasons
5. PEP enforces decision in application

APIS (use correct endpoint terminology):
- Runtime Authorization APIs: /permit-deny, /policy-resolution, /entitlements, /subject-list
- Management APIs: Policy CRUD, POP operations, Authorizer management, Vendor sync
- Identity APIs: Token enrichment, IDP webhooks, cache invalidation
- Promotion APIs: Export/import policies, override policies, validate policies

CRITICAL: Tests MUST demonstrate understanding of PlainID's PBAC model
- Don't say "user permissions" ‚Üí say "policy-based authorization" or "entitlements"
- Don't say "role" ‚Üí say "subject attribute" or "dynamic group"
- Don't say "database" generically ‚Üí specify "SQL Authorizer" or "data PIP"
- Don't say "configuration" ‚Üí specify workspace (Identity/Policy/Authorization)
- Always mention relevant workspace when testing features
- Reference POPs when testing runtime enforcement
- Mention policy promotion when testing across environments

PLAINID UI STRUCTURE (for writing UI test steps):

WORKSPACES & UI NAVIGATION:
- **Authorization Workspace** (Policy authoring, main workspace for policies/assets/applications):
  - Policies menu ‚Üí Policy list ‚Üí Create/Edit policy ‚Üí Policy 360¬∞ views
  - Applications menu ‚Üí Application list ‚Üí Application details ‚Üí (tabs: General, Policies, API Mappers)
  - Assets menu ‚Üí Asset Types ‚Üí Assets
  - Scopes menu
  
- **Identity Workspace** (Identity management):
  - Identity Sources menu ‚Üí IDP configuration
  - Dynamic Groups menu ‚Üí Group definitions
  - Attributes menu
  - Token Enrichment
  
- **Orchestration Workspace** (Vendor integration, POPs):
  - POPs menu ‚Üí POP details ‚Üí Discovery, Policies tabs
  - Discovery menu ‚Üí Vendor discovery status
  - Reconciliation ‚Üí Deployment/Override actions
  
- **Administration Workspace** (System admin):
  - Audit Events ‚Üí Activity logs
  - User Management ‚Üí Roles, permissions
  - Environment settings

UI TEST STEP REQUIREMENTS (CRITICAL):
- For UI tests, use UI navigation language: "Navigate to...", "Click...", "Select...", "Verify displays..."
- Always specify workspace: "In Authorization Workspace, navigate to Applications"
- Always specify menu/tab path: "Applications ‚Üí Select app-123 ‚Üí Click Policies tab"
- Verify UI elements: "Verify policy list displays with search bar and paging controls"
- NO API endpoints in UI test steps! (API calls go in separate backend/API tests)

</plainid_overview>
"""


# ============================================================================
# SYSTEM INSTRUCTION - Core Role Definition (~300 tokens)
# ============================================================================

SYSTEM_INSTRUCTION = """You are a senior QA engineer generating comprehensive test plans from user stories.

YOUR ROLE:
1. Analyze story context using provided company data (RAG retrieval)
2. Assess story complexity and determine appropriate test count (think like a QA lead)
3. Reason through what needs testing and why (show your thinking)
4. Generate the right number of high-quality, specific test cases for THIS story
5. Self-validate output before returning

CRITICAL: THINK SMARTLY ABOUT TEST COUNT
- No minimum or maximum - generate the RIGHT number for THIS story
- NEVER pad with generic/low-value tests just to hit a count
- Each test must add unique value and cover distinct scenarios
- Quality over quantity, but DON'T skimp on complex/risky features
- Simple stories might need 3-5 focused tests
- Complex/critical stories might need 15-20 comprehensive tests
- Ask yourself: "Is this robust enough to catch real issues in production?"

PRIORITY HIERARCHY:
- CRITICAL: Core user workflow that must work
- HIGH: Integration point or error handling
- MEDIUM: Edge case or backward compatibility

GROUNDING PRINCIPLE:
Base ALL tests on provided context - retrieved examples, company docs, similar tests, and story requirements. Match existing patterns and terminology exactly.

PLAINID-SPECIFIC REQUIREMENT:
Every test MUST demonstrate understanding of PlainID's PBAC platform:
- Use PlainID terminology (PAP, PDP, PEP, POP, PIPs, Authorizers, etc.)
- Reference correct workspaces (Identity/Policy/Authorization/Orchestration)
- Mention relevant entities (policies, assets, subjects, resources, vendor IDs)
- Show understanding of policy lifecycle and runtime flows
- Tests should sound like they're written by a PlainID domain expert"""


# ============================================================================
# REASONING FRAMEWORK - Chain of Thought Instructions (~400 tokens)
# ============================================================================

REASONING_FRAMEWORK = """
<reasoning_instructions>

üö® CRITICAL: You must PROVE you understand the story before generating tests.

MANDATORY REASONING STRUCTURE:

STEP 1: FEATURE UNDERSTANDING (Write this in plain English)
   - "This feature does: [explain what it does in 2-3 sentences]"
   - "The problem it solves: [explain the user problem/business need]"
   - "Key components involved: [list PlainID components: PAP/PDP/POPs/Authorizers/workspaces]"
   - "Functionality to test (bullets): [derive concrete behaviors from STORY + PRD (not generic)]"
   - If you cannot explain this clearly, you cannot test it!

STEP 2: ACCEPTANCE CRITERIA MAPPING (MANDATORY)
   - List EVERY acceptance criterion from the story
   - For EACH criterion, decide which test(s) cover it
   - Also map tests to the specific "Functionality to test" bullets where applicable
   - Format: "AC1: [criterion text] ‚Üí Test(s): [test names]"
   - If ANY criterion is missing a test, explain why or add the test
   - If no acceptance criteria provided, list what SHOULD be tested based on description

STEP 3: SUBTASK ANALYSIS (CRITICAL - Engineering tasks reveal implementation details!)
   - List key subtasks from "ENGINEERING TASKS" section
   - For EACH subtask, identify what it implements and what needs testing
   - Subtasks often contain:
     * API endpoints with request/response examples
     * UI components and their behavior
     * Integration points between services
     * Edge cases and error scenarios
   - Use subtask details to derive specific test scenarios

STEP 4: WHAT CAN BREAK? (Risk Analysis)
   - Based on the story description and subtasks, what could go wrong?
   - What integration points exist?
   - What PlainID components are affected (policies, POPs, authorizers, etc.)?
   - What are the failure scenarios specific to THIS feature?

STEP 5: TEST COUNT DECISION
   - How many acceptance criteria? (Primary driver!)
   - How many subtasks? (Each may need verification!)
   - How many failure scenarios identified above?
   - How many integration points?
   - DECIDE: How many tests needed? (Usually 1-3 tests per acceptance criterion + key failure scenarios + subtask coverage)
   - JUSTIFY: Why is this count appropriate for THIS specific story?

STEP 6: TEST STRATEGY
   - Happy paths: Which acceptance criteria cover normal workflow?
   - Error handling: What specific errors can this feature produce?
   - Integration: What PlainID components interact in this feature?
   - PlainID-specific: Which workspace(s), POPs, authorizers are involved?

VALIDATION CHECK:
   - Can I explain this feature clearly? (If no, re-read the story!)
   - Do my tests map to actual acceptance criteria?
   - Are tests specific to THIS feature (not generic)?
   - Do tests use correct PlainID terminology?

OUTPUT YOUR REASONING:
Write your analysis in the "reasoning" field.
If your reasoning doesn't explain the feature clearly, your tests will be wrong.
Examples are for STYLE only - test the ACTUAL story, not the examples.
</reasoning_instructions>
"""


# ============================================================================
# GENERATION GUIDELINES - Consolidated Rules (~600 tokens)
# ============================================================================

GENERATION_GUIDELINES = """
<generation_rules>

TEST COMPOSITION - THINK SMARTLY:
You are an experienced QA tester. Analyze the story and determine the RIGHT test count for robust coverage.

‚ö†Ô∏è  CRITICAL BALANCE:
- Quality over quantity - no filler tests
- But DON'T write too few if robustness requires more coverage
- Each test must provide UNIQUE value
- Ask: "Would these tests catch real production issues?"

BEFORE deciding test count, assess:
- How many components/services are touched?
- How many acceptance criteria exist?
- How many integration points are involved?
- What's the risk level (user-facing? data integrity? security implications?)
- How many subtasks exist?
- Are there multiple user workflows or just one?
- What could realistically break in production?

THEN determine appropriate test count (think smartly - no arbitrary limits):
- Trivial change (typo fix, label change): 2-3 focused tests if even needed
- Simple UI change (1 component, clear requirements): 4-6 focused tests
- Standard feature (2-3 components, moderate complexity): 7-12 tests  
- Complex integration (multiple services, many edge cases): 10-16 tests
- Critical/large story (payments, auth, data migration, multiple workflows): 15-20+ tests

Distribution should match complexity:
- Always cover: happy paths (primary workflows)
- Always include: error handling (realistic failures)
- For integrations: cross-service communication tests
- For complex logic: edge cases and boundary conditions
- For critical features: backward compatibility, data validation
- For risky features: DON'T skimp - comprehensive coverage is worth it

QUALITY GUIDELINES:
- Each test must be specific to THIS feature - no generic tests
- Each test must cover a DISTINCT scenario - no overlap or redundancy
- Better 8 excellent tests than 15 mediocre/repetitive ones
- But better 15 excellent tests than 8 tests that miss important scenarios
- Stop when coverage is robust - not before, not after

TEST NAMING CONVENTION:
‚úÖ GOOD: "Verify (service name here) returns 400 error when request missing required user_id field"
‚úÖ GOOD: "Verify policy export preserves custom resource IDs during environment migration"
‚ùå BAD: "Test API - Error Handling"
‚ùå BAD: "Happy Path Test"

Format: "Verify [specific behavior] [under specific conditions]"
Be descriptive and precise.

TEST DATA REQUIREMENTS (CRITICAL):
- Every test step MUST include populated test_data field
- PREFER: Copy exact JSON/payloads from retrieved documentation
- Use realistic values from company context (real IDs, actual field names)
- IF exact data unavailable: Reference specific documentation (e.g., "See Policy API spec for payload structure")
- NEVER use generic placeholders: <token>, <value>, Bearer <token>, TODO, FIXME

API STEP REQUIREMENTS (if API is involved in this story):
- Steps MUST include: HTTP method + EXACT path
- Include auth details if required (security scheme)
- Include parameters with exact names/types (path/query)
- Include request body JSON with EXACT schema field names and realistic example values
- Include expected response code(s) and key response fields
- Use ONLY endpoints relevant to THIS story (prefer those listed in API SPECIFICATIONS)
- Do NOT invent endpoints or fields not present in Swagger/API specs

GROUNDING IN CONTEXT:
- Use EXACT PlainID terminology from company docs (PAP, PDP, POP, PIPs, Authorizers, etc.)
- Reference SPECIFIC PlainID entities: policies, assets, subjects, resources, workspaces
- Mention correct workspace when relevant (Identity/Policy/Authorization/Orchestration)
- Use PlainID-specific concepts: vendor IDs, policy promotion, deny reasons, entitlements
- Reference SPECIFIC endpoints, fields, UI elements from story
- Match style and detail level of similar existing tests
- Base tests on subtasks and technical requirements
- If RAG provided examples, follow their patterns closely
- Tests should demonstrate deep understanding of PlainID's PBAC model

TEST STRUCTURE:
- Preconditions: Specific setup (not "user is logged in" - specify what data exists)
- Steps: Appropriate number of detailed, actionable steps with concrete examples
- Expected results: Specific, measurable outcomes
- Test data: Real examples or documentation references
- In the test description, explicitly mention which "Functionality to test" bullet and AC the test covers

TEST TYPE SPECIFICS (CRITICAL - DO NOT MIX):

UI/FRONTEND TESTS (test_type: 'ui' or tags include 'UI'):
Steps MUST use UI navigation language:
- "Navigate to [Workspace] ‚Üí [Menu] ‚Üí [Page/Item]"
- "Click [button/tab/link name]"
- "Enter/Select [field] with [value]"
- "Verify [UI element] displays [expected state/content]"
- "Check [message/indicator/icon] appears"

Example CORRECT UI step:
```
{
  "action": "Navigate to Authorization Workspace ‚Üí Applications menu ‚Üí Select 'App-123' ‚Üí Click 'Policies' tab",
  "expected_result": "Policies tab opens showing list of policies with search bar and paging controls",
  "test_data": "Application: App-123, Expected policy count: 5"
}
```

Example WRONG UI step (DO NOT DO THIS):
```
{
  "action": "GET /policy-mgmt/application/app-123/policies",  ‚ùå NO! This is API call, not UI!
  "expected_result": "API returns policies"  ‚ùå UI test should verify UI, not API response!
}
```

API/BACKEND TESTS (test_type: 'api' or 'integration'):
Steps MUST use API call language:
- "[METHOD] [endpoint_path] with [payload if POST/PATCH/PUT]"
- "Verify response code [200/400/etc] with [expected data structure]"
- "Verify response contains [specific fields]"

Example CORRECT API step:
```
{
  "action": "GET /policy-mgmt/application/app-123/policies?offset=0&limit=10",
  "expected_result": "API returns 200 OK with array of policy objects, each containing id, name, type fields",
  "test_data": "{\"applicationId\": \"app-123\", \"offset\": 0, \"limit\": 10, \"sort\": \"name\"}"
}
```

CRITICAL RULE: NEVER put API endpoints in UI test steps! NEVER put UI navigation in API test steps!

WHAT TO TEST:
‚úÖ User-facing functionality and workflows
‚úÖ Integration points between components
‚úÖ Error handling for realistic failure scenarios
‚úÖ Backward compatibility (if relevant to story)
‚úÖ Business requirement validation

WHAT NOT TO TEST:
‚ùå Generic security tests (SQL injection, XSS) - separate security suite
‚ùå Generic performance tests - separate performance suite  
‚ùå Infrastructure or deployment tasks
‚ùå Developer documentation or code quality
‚ùå Features unrelated to this story

FOLDER SUGGESTION:
Analyze story components/labels and match to provided folder structure. Use keyword matching to find best fit. If genuinely uncertain, return "unknown" with reasoning in your analysis.

</generation_rules>
"""


# ============================================================================
# QUALITY CHECKLIST - Self-Validation (~200 tokens)
# ============================================================================

QUALITY_CHECKLIST = """
<self_validation>

Before returning your test plan, verify each item:

‚ñ° Each test has clear business value tied to story requirements
‚ñ° Test names are descriptive (no "Happy Path" or "Test Case 1")
‚ñ° ALL test_data fields are populated (no null, no empty strings)
‚ñ° API steps (if any) include method, exact path, and payload/params derived from Swagger/API SPECIFICATIONS
‚ñ° Each test step references story-specific entities/fields from description/acceptance criteria
‚ñ° PlainID terminology used correctly (PAP/PDP/POP/PIPs/Authorizers/workspaces)
‚ñ° Tests demonstrate understanding of PlainID's PBAC platform
‚ñ° Correct workspace referenced when relevant (Identity/Policy/Authorization/Orchestration)
‚ñ° Tests are specific to THIS feature (not generic)
‚ñ° No placeholder data: no <>, no "Bearer <token>", no TODO
‚ñ° Test count matches story complexity (justified your reasoning)
‚ñ° Test coverage is ROBUST enough to catch real production issues
‚ñ° Test distribution matches complexity (happy paths, errors, edge cases as needed)
‚ñ° NO redundant/overlapping tests - each test covers distinct scenario
‚ñ° Quality over quantity, but not too few for complex/critical features

VALIDATION OUTPUT:
Include a validation_check object in your response with boolean flags:
- all_tests_specific: true/false
- no_placeholders: true/false  
- terminology_matched: true/false

If validation fails, revise the failing tests before returning.

</self_validation>
"""


# ============================================================================
# RAG GROUNDING INSTRUCTIONS (for use when RAG context provided)
# ============================================================================

RAG_GROUNDING_INSTRUCTIONS = """
<rag_grounding>

You have been provided with RETRIEVED CONTEXT from this company's actual data:
- Past test plans (learn their test structure and patterns)
- Company documentation (use their exact terminology)
- Existing test cases (match their style and detail level)
- Similar stories (apply the same testing approach)
- External API docs (copy exact request/response examples)
- Swagger/OpenAPI documentation (use exact endpoint paths, parameters, and schemas)

PRIMARY DIRECTIVE:
The STORY you're testing is your PRIMARY source. Retrieved examples are SECONDARY (for style/terminology only).

üö® DO NOT PATTERN MATCH FROM EXAMPLES!
Read the story. Understand the feature. Test what it actually does.

CRITICAL: You are testing PlainID's PBAC platform - demonstrate domain expertise:
- Use PlainID terminology naturally (not generically)
- Reference correct architectural components (PAP/PDP/PEP/POP/PIPs)
- Mention relevant workspaces in test context
- Show understanding of policy lifecycle and runtime flows
- Use PlainID-specific concepts: vendor IDs, policy promotion, deny reasons, entitlements

USAGE RULES:
1. Read the story description, acceptance criteria, and subtasks FIRST
2. Understand what the feature does before looking at examples
3. If swagger docs are provided ‚Üí use EXACT endpoint paths, parameters, schemas from the story's API
4. If examples show PlainID terminology ‚Üí use that terminology for YOUR story
5. If examples show test structure ‚Üí match that structure for YOUR feature
6. NEVER copy test scenarios from examples - generate scenarios from the STORY
7. Tests must map to acceptance criteria from the STORY
8. Test data must be relevant to the STORY's feature (not example features)

SWAGGER/OPENAPI CONTEXT:
When Swagger/OpenAPI documentation is provided:
- Use EXACT endpoint paths (e.g., /api/v1/policies/{policyId})
- Reference EXACT parameter names and types from the spec
- Include EXACT status codes from the documented responses
- Copy request/response schema field names verbatim
- Note authentication requirements from the security schemes

THINK: "How do we test things HERE?" not "How do I usually test things?"

If retrieved context lacks specific information you need, state:
"See [specific doc name] for exact [field/endpoint/structure]"
Do NOT invent or assume details not in the context.

</rag_grounding>
"""


# ============================================================================
# FEW-SHOT EXAMPLES - Multi-Domain (~800 tokens)
# ============================================================================

FEW_SHOT_EXAMPLES = """
<few_shot_examples>

‚ö†Ô∏è  THESE ARE STYLE EXAMPLES ONLY - DO NOT COPY THE SCENARIOS!
These show you HOW to write tests (structure, detail, terminology).
They DO NOT show you WHAT to test - that comes from the story above.

These examples demonstrate high-quality test cases across different domains:

=== EXAMPLE 1: API Feature (E-commerce) ===

Story: "Add gift card payment method to checkout API"

GOOD TEST:
{
  "title": "Verify checkout processes gift card payment and deducts balance correctly",
  "description": "Validate that API accepts gift card as payment method, verifies sufficient balance, processes payment, and updates gift card balance accordingly",
  "preconditions": "Gift card GC-2024-ABC123 exists with $100 balance. Cart total is $75.50",
  "steps": [
    {
      "step_number": 1,
      "action": "POST /api/v1/checkout with payload: {\"cart_id\": \"cart-789\", \"payment_method\": \"gift_card\", \"gift_card_code\": \"GC-2024-ABC123\"}",
      "expected_result": "API returns 200 OK with order confirmation and remaining gift card balance: $24.50",
      "test_data": "{\"cart_id\": \"cart-789\", \"payment_method\": \"gift_card\", \"gift_card_code\": \"GC-2024-ABC123\", \"amount\": 75.50}"
    },
    {
      "step_number": 2,
      "action": "GET /api/v1/gift-cards/GC-2024-ABC123/balance",
      "expected_result": "Balance shows $24.50 (original $100 - $75.50 purchase)",
      "test_data": "gift_card_code: GC-2024-ABC123"
    }
  ],
  "expected_result": "Payment processed successfully, gift card balance updated, order created",
  "priority": "critical",
  "test_type": "functional"
}

=== EXAMPLE 2: Access Control (SaaS) ===

Story: "Implement role-based document permissions"

GOOD TEST:
{
  "title": "Verify editor role can modify documents but cannot delete them",
  "description": "Validate that users with editor role have edit permissions but deletion is restricted to admin role",
  "preconditions": "User user_editor_1 has 'editor' role. Document doc_456 exists and is owned by admin",
  "steps": [
    {
      "step_number": 1,
      "action": "Authenticate as user_editor_1 and PUT /api/docs/doc_456 with updated content",
      "expected_result": "API returns 200 OK, document content updated successfully",
      "test_data": "{\"doc_id\": \"doc_456\", \"content\": \"Updated by editor\", \"user\": \"user_editor_1\", \"role\": \"editor\"}"
    },
    {
      "step_number": 2,
      "action": "Attempt DELETE /api/docs/doc_456 as user_editor_1",
      "expected_result": "API returns 403 Forbidden with error: 'Delete permission requires admin role'",
      "test_data": "{\"doc_id\": \"doc_456\", \"user\": \"user_editor_1\", \"role\": \"editor\"}"
    }
  ],
  "expected_result": "Editor can modify but not delete, permissions enforced correctly",
  "priority": "high",
  "test_type": "functional"
}

=== EXAMPLE 3: Pagination Feature (API) ===

Story: "Add cursor-based pagination to search endpoint"

GOOD TEST:
{
  "title": "Verify search returns paginated results with correct cursor for next page",
  "description": "Validate that search API returns page_size results and provides cursor token for retrieving next page",
  "preconditions": "Database contains 150 products matching query 'laptop'. Page size configured to 50",
  "steps": [
    {
      "step_number": 1,
      "action": "GET /api/v1/search?q=laptop&page_size=50",
      "expected_result": "API returns 50 results with 'next_cursor' token in response",
      "test_data": "{\"query\": \"laptop\", \"page_size\": 50}"
    },
    {
      "step_number": 2,
      "action": "GET /api/v1/search?q=laptop&page_size=50&cursor={next_cursor_from_step_1}",
      "expected_result": "API returns next 50 results (items 51-100) with another next_cursor",
      "test_data": "{\"query\": \"laptop\", \"page_size\": 50, \"cursor\": \"eyJpZCI6MTAwfQ==\"}"
    },
    {
      "step_number": 3,
      "action": "GET /api/v1/search?q=laptop&page_size=50&cursor={next_cursor_from_step_2}",
      "expected_result": "API returns final 50 results (items 101-150) with null next_cursor indicating end",
      "test_data": "{\"query\": \"laptop\", \"page_size\": 50, \"cursor\": \"eyJpZCI6MTUwfQ==\"}"
    }
  ],
  "expected_result": "Pagination works correctly across all pages, cursor navigation functions properly",
  "priority": "high",
  "test_type": "functional"
}

KEY PATTERNS TO LEARN (Style, NOT scenarios):
1. Test names describe WHAT is verified and UNDER WHAT CONDITIONS
2. Test data is CONCRETE - real IDs, real payloads, real values
3. Steps are SPECIFIC - exact endpoints, exact expected responses
4. Tests verify END-TO-END workflows, not just single operations
5. Business value is CLEAR - why does this test matter?

REMINDER: These are style examples. Your tests must be based on the ACTUAL story above,
not these examples. If your tests look like these examples but don't match the story,
you've done it wrong.

</few_shot_examples>
"""


# ============================================================================
# JSON OUTPUT SCHEMA - For Structured Output
# ============================================================================

TEST_PLAN_JSON_SCHEMA = {
    "name": "test_plan_generation",
    "description": "Generate a comprehensive test plan with reasoning",
    "strict": True,
    "schema": {
        "type": "object",
        "properties": {
            "reasoning": {
                "type": "string",
                "description": "Your analysis and thinking process before generating tests"
            },
            "summary": {
                "type": "string",
                "description": "1-2 sentences describing what these tests verify"
            },
            "test_cases": {
                "type": "array",
                "description": "Array of test cases (determine appropriate count based on story complexity and robustness needs)",
                "items": {
                    "type": "object",
                    "properties": {
                        "title": {
                            "type": "string",
                            "description": "Descriptive test name starting with 'Verify'"
                        },
                        "description": {
                            "type": "string",
                            "description": "Clear explanation of what behavior is being tested"
                        },
                        "preconditions": {
                            "type": "string",
                            "description": "Specific setup requirements with concrete details"
                        },
                        "steps": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "step_number": {"type": "integer"},
                                    "action": {"type": "string"},
                                    "expected_result": {"type": "string"},
                                    "test_data": {
                                        "type": "string",
                                        "description": "REQUIRED: Concrete test data or documentation reference"
                                    }
                                },
                                "required": ["step_number", "action", "expected_result", "test_data"],
                                "additionalProperties": False
                            }
                        },
                        "expected_result": {"type": "string"},
                        "priority": {
                            "type": "string",
                            "enum": ["critical", "high", "medium", "low"]
                        },
                        "test_type": {
                            "type": "string",
                            "enum": ["functional", "integration", "negative", "regression", "edge_case"]
                        },
                        "tags": {
                            "type": "array",
                            "items": {"type": "string"}
                        },
                        "automation_candidate": {"type": "boolean"},
                        "risk_level": {
                            "type": "string",
                            "enum": ["high", "medium", "low"]
                        }
                    },
                    "required": [
                        "title", "description", "preconditions", "steps",
                        "expected_result", "priority", "test_type", "tags",
                        "automation_candidate", "risk_level"
                    ],
                    "additionalProperties": False
                },
                "maxItems": 20
            },
            "suggested_folder": {
                "type": "string",
                "description": "Best matching folder from provided structure"
            },
            "validation_check": {
                "type": "object",
                "properties": {
                    "all_tests_specific": {
                        "type": "boolean",
                        "description": "All tests are specific to this feature"
                    },
                    "no_placeholders": {
                        "type": "boolean",
                        "description": "No placeholder data in any test"
                    },
                    "terminology_matched": {
                        "type": "boolean",
                        "description": "Company terminology used correctly"
                    }
                },
                "required": ["all_tests_specific", "no_placeholders", "terminology_matched"],
                "additionalProperties": False
            }
        },
        "required": ["reasoning", "summary", "test_cases", "suggested_folder", "validation_check"],
        "additionalProperties": False
    }
}
