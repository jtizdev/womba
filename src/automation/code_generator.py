"""
Automated test code generation from test plans.
Uses AI code generation tools (cursor-cli, aider) to analyze repo and generate matching code.
"""

import json
import subprocess
from pathlib import Path
from typing import Dict, List, Optional
from loguru import logger

from src.models.test_plan import TestPlan
from src.models.test_case import TestCase
from .framework_detector import FrameworkDetector
from .pr_creator import PRCreator


class TestCodeGenerator:
    """Generates executable test code from test plans."""

    def __init__(
        self,
        repo_path: str,
        framework: str = "auto",
        ai_tool: str = "aider"  # or "cursor-cli"
    ):
        """
        Args:
            repo_path: Path to customer's test repository
            framework: Test framework (auto, playwright, cypress, rest-assured, junit)
            ai_tool: AI code generation tool to use
        """
        self.repo_path = Path(repo_path)
        self.framework = framework
        self.ai_tool = ai_tool

        if not self.repo_path.exists():
            raise ValueError(f"Repository path does not exist: {repo_path}")

        self.detector = FrameworkDetector(repo_path)
        self.pr_creator = PRCreator(repo_path)

    async def analyze_repo(self) -> Dict[str, any]:
        """
        Analyze repo to detect:
        - Test framework used
        - File naming patterns
        - Code structure
        - Import patterns

        Returns:
            Dictionary with repository analysis
        """
        logger.info("Analyzing repository structure and patterns...")

        # Detect framework if auto
        if self.framework == "auto":
            self.framework = self.detector.detect_framework()
            logger.info(f"Auto-detected framework: {self.framework}")

        # Analyze patterns
        patterns = self.detector.analyze_patterns()

        analysis = {
            "framework": self.framework,
            "naming_pattern": patterns["naming_pattern"],
            "directory_structure": patterns["directory_structure"],
            "import_patterns": patterns["import_patterns"][:10],  # Top 10
            "test_structure": patterns["test_structure"]
        }

        logger.info(f"Repository analysis complete: {analysis['framework']} framework detected")
        return analysis

    async def generate_code(
        self,
        test_plan: TestPlan,
        target_branch: str = None,
        create_pr: bool = True
    ) -> Optional[str]:
        """
        Generate test code matching repo patterns and optionally create PR.

        Args:
            test_plan: Test plan with test cases to implement
            target_branch: Branch name (default: feature/ai-tests-{story_key})
            create_pr: Whether to create a PR automatically

        Returns:
            PR URL if create_pr=True, otherwise branch name
        """
        story_key = test_plan.story.key
        if not target_branch:
            target_branch = f"feature/ai-tests-{story_key.lower()}"

        logger.info(f"Generating test code for {len(test_plan.test_cases)} test cases")

        # Step 1: Analyze repo
        repo_analysis = await self.analyze_repo()

        # Step 2: Generate prompt for AI tool
        prompt = self._build_generation_prompt(test_plan, repo_analysis)

        # Step 3: Use AI tool to generate code
        generated_files = await self._generate_with_ai_tool(prompt, test_plan)

        if not generated_files:
            logger.warning("No test files generated by AI tool")
            return []

        # Step 4: Create branch and commit
        if not self.pr_creator.create_branch(target_branch):
            logger.error("Failed to create branch")
            return []

        commit_message = self._build_commit_message(test_plan)
        if not self.pr_creator.commit_files(generated_files, commit_message):
            logger.error("Failed to commit files")
            return []

        # Step 5: Push branch
        if not self.pr_creator.push_branch(target_branch):
            logger.error("Failed to push branch")
            return []

        # Step 6: Create PR if requested
        if create_pr:
            pr_url = self.pr_creator.create_pr(
                test_plan=test_plan,
                branch_name=target_branch
            )
            return pr_url
        else:
            return target_branch

    def _build_generation_prompt(
        self,
        test_plan: TestPlan,
        repo_analysis: Dict[str, any]
    ) -> str:
        """Build detailed prompt for AI code generation tool."""
        story = test_plan.story
        test_cases = test_plan.test_cases
        
        # Analyze what kind of tests these are
        test_types = set(tc.test_type for tc in test_cases)
        feature_area = self._extract_feature_area(story.summary, story.description)

        prompt = f"""# Task: Analyze Repository and Generate Automated Test Files

You are an expert test automation engineer working on a Java/TestNG/REST Assured automation repository.

## CRITICAL INSTRUCTIONS - READ FIRST

**BEFORE WRITING ANY CODE:**
1. **ANALYZE the repository** to find existing test infrastructure for "{feature_area}"
2. **SEARCH** for similar test files (Policy, Authorization, Management, etc.)
3. **IDENTIFY** existing base classes, utilities, builders, and helpers
4. **CHECK** if infrastructure exists for this feature area

**IF infrastructure EXISTS:**
- Add test methods to the appropriate existing test class
- Follow the exact pattern of existing tests in that class
- Use existing builders and utilities

**IF infrastructure DOES NOT exist:**
- Create NEW infrastructure (test class, builder, helper) following repository patterns
- Study similar feature areas (e.g., if creating Policy tests, look at how Application tests are structured)
- Copy the architectural pattern (BaseTest, Builder pattern, utility classes)
- Create all necessary infrastructure files before writing tests

## Story Context
- **Story Key**: {story.key}
- **Summary**: {story.summary}
- **Feature Area**: {feature_area}
- **Description**: 
{story.description[:800] if story.description else 'No description'}

## Repository Information
- **Framework**: {repo_analysis['framework']} (Java/TestNG/REST Assured/Maven)
- **Architecture Pattern**: Page Object Model with Builder pattern
- **Base Classes**: Look for BaseTest, BaseBuilder, BaseHelper
- **Test Location**: `src/test/java/com/plainid/auto/test/`
- **Builders Location**: `src/main/java/com/plainid/auto/builders/`
- **Utilities**: RestAssuredManager, AllureStepLogger, TestContext

## Common Repository Patterns (FOLLOW THESE EXACTLY)

### 1. Test Class Structure
```java
@Epic("Feature Area")
@Feature("Specific Feature")
public class FeatureNameTest extends BaseTest {{
    
    @Test(description = "Test description")
    @Severity(SeverityLevel.CRITICAL)
    @Story("Story Key")
    public void testMethodName() {{
        // Arrange
        // Act  
        // Assert
    }}
}}
```

### 2. Builder Pattern (if needed)
```java
public class FeatureBuilder extends BaseBuilder {{
    public FeatureBuilder(RestAssuredManager manager) {{
        super(manager);
    }}
    
    public String createFeature(FeatureRequest request) {{
        // Implementation
    }}
}}
```

### 3. Common Imports (ALWAYS USE THESE)
```java
import io.qameta.allure.*;
import io.restassured.response.Response;
import org.testng.annotations.Test;
import static org.testng.Assert.*;
import com.plainid.auto.utils.RestAssuredManager;
```

## Test Cases to Implement

{self._format_test_cases_for_prompt(test_cases)}

## Step-by-Step Instructions

### Step 1: Repository Analysis (REQUIRED)
```
Search for existing files containing: "{feature_area}"
Look for patterns in: src/test/java/com/plainid/auto/test/
Check builders in: src/main/java/com/plainid/auto/builders/
Identify similar feature test structure
```

### Step 2: Decision Point
**ASK YOURSELF:** "Does infrastructure for '{feature_area}' already exist?"

**IF YES (infrastructure exists):**
- Open the existing test class
- Add new @Test methods following exact same pattern
- Use existing builders and utilities
- Match the coding style exactly

**IF NO (new infrastructure needed):**
- Create test class: `src/test/java/com/plainid/auto/test/{feature_area.lower()}/{story.key.replace('-', '')}Test.java`
- Create builder: `src/main/java/com/plainid/auto/builders/{feature_area.lower()}/{feature_area}Builder.java`
- Follow the exact pattern from similar features
- Extend BaseTest, use Allure annotations
- Use RestAssuredManager for API calls

### Step 3: Implementation Requirements

1. **File Naming**: Use story key + descriptive name (e.g., `PLAT15596VendorCompareViewTest.java`)
2. **Package**: Place in appropriate package under `com.plainid.auto.test`
3. **Annotations**: 
   - @Epic for feature area
   - @Feature for specific capability
   - @Story for story key
   - @Severity for priority
4. **Test Methods**:
   - Descriptive names (e.g., `testVendorCompareViewDisplaysSyncStatus`)
   - Clear Arrange-Act-Assert pattern
   - Proper assertions with messages
5. **Builders**: Create request builders if API calls are needed
6. **Utilities**: Use existing RestAssuredManager, don't create new HTTP clients
7. **Test Data**: Use realistic data from test case specifications
8. **Assertions**: Specific assertions (assertEquals, assertTrue, assertNotNull)

### Step 4: Code Quality Checklist
- [ ] Follows repository conventions (file structure, naming, imports)
- [ ] Uses existing infrastructure (builders, utilities, base classes)
- [ ] Has proper Allure annotations for reporting
- [ ] Includes clear test descriptions
- [ ] Has appropriate assertions with error messages
- [ ] Is independent (doesn't rely on test execution order)
- [ ] Compiles successfully with Maven
- [ ] Follows Java best practices

## Important Notes

⚠️ **DO NOT:**
- Create generic test files without analyzing the repository
- Ignore existing patterns and infrastructure
- Use different naming conventions
- Skip Allure annotations
- Create standalone tests without using BaseTest
- Hardcode URLs or credentials (use config/properties)

✅ **DO:**
- Search and analyze existing code first
- Reuse existing builders and utilities
- Follow exact patterns from similar tests
- Use proper annotations for test reporting
- Make tests independent and reliable
- Add meaningful assertions
- Consider both positive and negative scenarios
- Handle edge cases mentioned in test cases

## Expected Output

Create production-ready test file(s) that:
1. Implement ALL {len(test_cases)} test cases specified above
2. Follow repository patterns exactly
3. Use existing infrastructure where available
4. Create new infrastructure only if necessary (following existing patterns)
5. Are ready to run with `mvn test`
6. Generate proper Allure reports

**Remember**: Quality over quantity. One well-structured test following repository patterns is better than multiple generic tests that don't match the codebase.
"""

        return prompt

    def _extract_feature_area(self, summary: str, description: str = "") -> str:
        """Extract the feature area from story summary/description."""
        # Common feature areas in the automation repo
        feature_keywords = {
            'policy': 'Policy',
            'authorization': 'Authorization',
            'application': 'Application',
            'identity': 'Identity',
            'template': 'IdentityTemplate',
            'pop': 'POP',
            'vendor': 'Vendor',
            'compare': 'Compare',
            'porch': 'PORCH',
            'pap': 'PAP',
            'pdp': 'PDP',
            'audit': 'Audit',
            'management': 'Management',
            '360': 'Policy360'
        }
        
        text = (summary + " " + (description or "")).lower()
        
        # Look for matches
        for keyword, area in feature_keywords.items():
            if keyword in text:
                return area
        
        # Default to extracting from summary
        parts = summary.split('-')
        if len(parts) >= 2:
            return parts[1].strip().replace(' ', '')
        
        return "General"
    
    def _format_test_cases_for_prompt(self, test_cases: List[TestCase]) -> str:
        """Format test cases for the AI prompt."""
        formatted = []

        for i, tc in enumerate(test_cases, 1):
            formatted.append(f"""
### Test Case {i}: {tc.title}

**Description**: {tc.description}

**Type**: {tc.test_type}  
**Priority**: {tc.priority}  
**Preconditions**: {tc.preconditions or 'None'}

**Steps**:
""")
            for step in tc.steps:
                formatted.append(f"{step.step_number}. {step.action}")
                formatted.append(f"   **Expected**: {step.expected_result}")
                if step.test_data:
                    formatted.append(f"   **Test Data**: {step.test_data}")
                formatted.append("")

            formatted.append(f"**Expected Result**: {tc.expected_result}\n")

        return "\n".join(formatted)

    async def _generate_with_ai_tool(
        self,
        prompt: str,
        test_plan: TestPlan
    ) -> List[str]:
        """
        Use AI tool (aider or cursor-cli) to generate code.

        Args:
            prompt: Generation prompt
            test_plan: Test plan for context

        Returns:
            List of generated file paths
        """
        logger.info(f"Using {self.ai_tool} to generate test code...")

        if self.ai_tool == "aider":
            return await self._generate_with_aider(prompt)
        elif self.ai_tool == "cursor-cli":
            return await self._generate_with_cursor(prompt)
        else:
            logger.error(f"Unsupported AI tool: {self.ai_tool}")
            return []

    async def _generate_with_aider(self, prompt: str) -> List[str]:
        """
        Generate code using aider in customer's repo.
        Aider will analyze the repo and generate code matching their patterns.
        """
        try:
            # Save prompt to temp file in customer's repo
            prompt_file = self.repo_path / ".womba_prompt.txt"
            prompt_file.write_text(prompt)

            logger.info("Running aider to generate test code in customer's repo...")
            logger.info(f"Repo: {self.repo_path}")

            # Run aider in customer's repo - it will analyze their patterns automatically
            # Try python3 -m aider first, fallback to aider command
            aider_cmd = ["python3", "-m", "aider"]
            try:
                subprocess.run(["aider", "--version"], capture_output=True, check=True)
                aider_cmd = ["aider"]
            except (FileNotFoundError, subprocess.CalledProcessError):
                pass  # Use python3 -m aider
            
            result = subprocess.run(
                aider_cmd + [
                    "--yes",  # Auto-accept changes
                    "--no-git",  # We'll handle git ourselves
                    "--message-file", str(prompt_file)
                ],
                cwd=self.repo_path,  # Run in CUSTOMER'S repo
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )

            # Clean up prompt file
            if prompt_file.exists():
                prompt_file.unlink()

            if result.returncode != 0:
                logger.error(f"Aider failed: {result.stderr}")
                return []

            # Extract generated files from aider output
            # Aider outputs "Added: filename" or "Modified: filename"
            generated_files = []
            for line in result.stdout.split('\n'):
                if "Added:" in line or "Modified:" in line:
                    file_path = line.split(":", 1)[1].strip()
                    generated_files.append(file_path)

            logger.info(f"✅ Aider generated {len(generated_files)} test files")
            return generated_files

        except subprocess.TimeoutExpired:
            logger.error("Aider timed out after 5 minutes")
            return []
        except FileNotFoundError:
            logger.error("Aider not found. Install with: pip install aider-chat")
            return []
        except Exception as e:
            logger.error(f"Aider execution failed: {e}")
            return []

    async def _generate_with_cursor(self, prompt: str) -> List[str]:
        """
        Generate code using cursor-cli in customer's repo.
        Cursor will analyze the repo structure and generate matching code.
        """
        try:
            # Save prompt to temp file in customer's repo
            prompt_file = self.repo_path / ".womba_prompt.txt"
            prompt_file.write_text(prompt)

            logger.info("Running cursor-cli to generate test code in customer's repo...")
            logger.info(f"Repo: {self.repo_path}")

            # Run cursor-cli with composer/agent mode to generate code
            # The CLI will analyze the repo and generate code matching their patterns
            result = subprocess.run(
                [
                    "cursor",  # cursor-cli command
                    "--command", prompt,
                    "--path", str(self.repo_path),
                    "--no-interactive"  # Don't wait for user input
                ],
                cwd=self.repo_path,  # Run in CUSTOMER'S repo
                capture_output=True,
                text=True,
                timeout=300  # 5 minute timeout
            )

            # Clean up
            if prompt_file.exists():
                prompt_file.unlink()

            if result.returncode != 0:
                logger.error(f"cursor-cli failed: {result.stderr}")
                logger.info("Note: cursor-cli syntax may vary. Consider using 'aider' instead.")
                return []

            # Parse output to find generated files
            # Look for file creation messages in output
            generated_files = []
            for line in result.stdout.split('\n'):
                if "Created:" in line or "Modified:" in line or "Added:" in line:
                    file_path = line.split(":", 1)[1].strip()
                    generated_files.append(file_path)

            # If no files detected, try to find all modified files in git
            if not generated_files:
                git_result = subprocess.run(
                    ["git", "status", "--porcelain"],
                    cwd=self.repo_path,
                    capture_output=True,
                    text=True
                )
                for line in git_result.stdout.split('\n'):
                    if line.strip():
                        # Format: " M file.txt" or "?? file.txt"
                        file_path = line[3:].strip()
                        if file_path and 'test' in file_path.lower():
                            generated_files.append(file_path)

            logger.info(f"✅ Cursor generated {len(generated_files)} test files")
            return generated_files

        except subprocess.TimeoutExpired:
            logger.error("cursor-cli timed out after 5 minutes")
            return []
        except FileNotFoundError:
            logger.error("cursor command not found. Using aider as fallback...")
            return await self._generate_with_aider(prompt)
        except Exception as e:
            logger.error(f"cursor-cli execution failed: {e}")
            return []

    def _build_commit_message(self, test_plan: TestPlan) -> str:
        """Build commit message for generated tests."""
        story_key = test_plan.story.key
        test_count = len(test_plan.test_cases)

        message = f"""feat: Add AI-generated tests for {story_key}

Generated {test_count} test cases covering:
{test_plan.summary}

Test Types:
"""
        # Count test types
        test_types = {}
        for tc in test_plan.test_cases:
            test_types[tc.test_type] = test_types.get(tc.test_type, 0) + 1

        for test_type, count in test_types.items():
            message += f"- {count} {test_type} tests\n"

        message += f"\nGenerated by: Womba AI\nAI Model: {test_plan.metadata.ai_model}"

        return message

